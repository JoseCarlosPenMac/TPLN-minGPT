{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shows how one can generate text given a prompt and some hyperparameters, using either minGPT or huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from mingpt.model import GPT\n",
    "from mingpt.utils import set_seed\n",
    "from mingpt.bpe import BPETokenizer\n",
    "set_seed(3407)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_mingpt = True # use minGPT or huggingface/transformers model?\n",
    "model_type = 'gpt2-xl'\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 1557.61M\n"
     ]
    }
   ],
   "source": [
    "if use_mingpt:\n",
    "    model = GPT.from_pretrained(model_type)\n",
    "else:\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "    model.config.pad_token_id = model.config.eos_token_id # suppress a warning\n",
    "\n",
    "# ship model to device and set to eval mode\n",
    "model.to(device)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate(prompt='', num_samples=10, steps=20, do_sample=True):\n",
    "        \n",
    "    # tokenize the input prompt into integer input sequence\n",
    "    if use_mingpt:\n",
    "        tokenizer = BPETokenizer()\n",
    "        if prompt == '':\n",
    "            # to create unconditional samples...\n",
    "            # manually create a tensor with only the special <|endoftext|> token\n",
    "            # similar to what openai's code does here https://github.com/openai/gpt-2/blob/master/src/generate_unconditional_samples.py\n",
    "            x = torch.tensor([[tokenizer.encoder.encoder['<|endoftext|>']]], dtype=torch.long)\n",
    "        else:\n",
    "            x = tokenizer(prompt).to(device)\n",
    "    else:\n",
    "        tokenizer = GPT2Tokenizer.from_pretrained(model_type)\n",
    "        if prompt == '': \n",
    "            # to create unconditional samples...\n",
    "            # huggingface/transformers tokenizer special cases these strings\n",
    "            prompt = '<|endoftext|>'\n",
    "        encoded_input = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "        x = encoded_input['input_ids']\n",
    "    \n",
    "    # we'll process all desired num_samples in a batch, so expand out the batch dim\n",
    "    x = x.expand(num_samples, -1)\n",
    "\n",
    "    # forward the model `steps` times to get samples, in a batch\n",
    "    y = model.generate(x, max_new_tokens=steps, do_sample=do_sample, top_k=40)\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        out = tokenizer.decode(y[i].cpu().squeeze())\n",
    "        print('-'*80)\n",
    "        print(out)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Andrej Karpathy, the chief of the criminal investigation department, said during a news conference, \"We still have a lot of\n",
      "--------------------------------------------------------------------------------\n",
      "Andrej Karpathy, the man whom most of America believes is the architect of the current financial crisis. He runs the National Council\n",
      "--------------------------------------------------------------------------------\n",
      "Andrej Karpathy, the head of the Department for Regional Reform of Bulgaria and an MP in the centre-right GERB party\n",
      "--------------------------------------------------------------------------------\n",
      "Andrej Karpathy, the former head of the World Bank's IMF department, who worked closely with the IMF. The IMF had\n",
      "--------------------------------------------------------------------------------\n",
      "Andrej Karpathy, the vice president for innovation and research at Citi who oversaw the team's work to make sense of the\n",
      "--------------------------------------------------------------------------------\n",
      "Andrej Karpathy, the CEO of OOAK Research, said that the latest poll indicates that it won't take much to\n",
      "--------------------------------------------------------------------------------\n",
      "Andrej Karpathy, the former prime minister of Estonia was at the helm of a three-party coalition when parliament met earlier this\n",
      "--------------------------------------------------------------------------------\n",
      "Andrej Karpathy, the director of the Institute of Economic and Social Research, said if the rate of return is only 5 per\n",
      "--------------------------------------------------------------------------------\n",
      "Andrej Karpathy, the minister of commerce for Latvia's western neighbour: \"The deal means that our two countries have reached more\n",
      "--------------------------------------------------------------------------------\n",
      "Andrej Karpathy, the state's environmental protection commissioner. \"That's why we have to keep these systems in place.\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate(prompt='Andrej Karpathy, the', num_samples=10, steps=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from mingpt.bpe import BPETokenizer\n",
    "from mingpt.model import GPT\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Inicializar el modelo GPT y el tokenizer\n",
    "model = GPT.from_pretrained('gpt2')  # Asegúrate de cargar el modelo correcto\n",
    "model.eval()  # Modo evaluación\n",
    "bpe = BPETokenizer()\n",
    "\n",
    "# Texto limpio y tokenización\n",
    "clean_text = \"Michelle Jones was a top-notch student. Michelle\"\n",
    "print(f\"Clean text: {clean_text}\")\n",
    "tokens_clean = bpe(clean_text)[0]  # Tokeniza el texto\n",
    "input_length = tokens_clean.size(0)\n",
    "\n",
    "    # Ejecutar el modelo en el texto limpio\n",
    "with torch.no_grad():\n",
    "    logits, _ = model(tokens_clean.unsqueeze(0))  # Guarda activaciones\n",
    "    last_logits = logits[0, -1, :]  # Logits del último token\n",
    "    print(f\"Logits shape: {last_logits.shape}\")\n",
    "\n",
    "# Mostrar continuaciones más probables\n",
    "probabilities = torch.softmax(last_logits, dim=-1)\n",
    "top_k = 20  # Número de continuaciones más probables a mostrar\n",
    "top_indices = torch.topk(probabilities, k=top_k).indices\n",
    "top_probabilities = probabilities[top_indices]\n",
    "\n",
    "print(f\"Top {top_k} continuations:\")\n",
    "for i, index in enumerate(top_indices):\n",
    "    token_str = bpe.decode(torch.tensor([index.item()]))  # Decodificar token\n",
    "    prob = top_probabilities[i].item()\n",
    "    print(f\"{i + 1}: {token_str} ({prob:.4f})\")\n",
    "\n",
    "# Ejemplo para obtener la probabilidad de un token específico (e.g., \"Jones\")\n",
    "token_to_check = \" Jones\"\n",
    "index_to_check = bpe(token_to_check)[0][0].item()\n",
    "prob_of_token = probabilities[index_to_check].item()\n",
    "print(f\"Probability of '{token_to_check.strip()}': {prob_of_token:.4f}\")\n",
    "\n",
    "corrupt_text = \"Michelle Smith was a top-notch student. Michelle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_embeddings(model, tokenizer, clean_text, corrupted_text, target_tokens):\n",
    "    \"\"\"\n",
    "    Analyze the impact of embeddings on logits for a given corrupted text.\n",
    "\n",
    "    Args:\n",
    "        model: GPT model.\n",
    "        tokenizer: BPE tokenizer.\n",
    "        clean_text (str): Clean sequence.\n",
    "        corrupted_text (str): Corrupted sequence.\n",
    "        target_tokens (list): Tokens to analyze logits for (e.g., [\"Jones\", \"Smith\"]).\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Tokenize texts\n",
    "    clean_tokens = tokenizer(clean_text)[0]\n",
    "    corrupted_tokens = tokenizer(corrupted_text)[0]\n",
    "    input_tensor = corrupted_tokens.unsqueeze(0)\n",
    "\n",
    "    # Process clean sequence\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        model.forward(clean_tokens.unsqueeze(0), save_embeddings=True)\n",
    "        clean_logits = model.stored_logits  # Save logits from the clean sequence\n",
    "        print(f\"Logits shape: {clean_logits.shape}\")\n",
    "\n",
    "    # Mostrar continuaciones más probables\n",
    "    probabilities = torch.softmax(clean_logits, dim=-1)\n",
    "    top_k = 20  # Número de continuaciones más probables a mostrar\n",
    "    top_indices = torch.topk(probabilities, k=top_k).indices[0]\n",
    "    top_probabilities = probabilities[0,top_indices]\n",
    "\n",
    "    print(f\"Top {top_k} continuations:\")\n",
    "    for i, index in enumerate(top_indices):\n",
    "        token_str = bpe.decode(torch.tensor([index.item()]))  # Decodificar token\n",
    "        prob = top_probabilities[i].item()\n",
    "        print(f\"{i + 1}: {token_str} ({prob:.4f})\")\n",
    "\n",
    "    # Get target token indices\n",
    "    target_indices = [tokenizer(f\" {token}\")[0][0].item() for token in target_tokens]\n",
    "\n",
    "    # Initialize difference matrix\n",
    "    n_layers = len(model.transformer.h)\n",
    "    seq_len = input_tensor.size(1)\n",
    "    difference_matrix = np.zeros((n_layers, seq_len))\n",
    "\n",
    "    # Loop over layers and positions\n",
    "    for layer in range(n_layers):\n",
    "        for position in range(seq_len):\n",
    "            # Replace corrupted embedding with the clean embedding\n",
    "            intervened_embedding = model.stored_embeddings[layer][position]\n",
    "            model.forward(input_tensor, intervene_at=(layer, position, intervened_embedding))\n",
    "\n",
    "            # Get the logits from the intervened model\n",
    "            corrupted_logits = model.stored_logits\n",
    "\n",
    "            # Calculate the difference between clean and intervened logits for the target token\n",
    "            logit_diff = (corrupted_logits[0, target_indices[0]] - corrupted_logits[0, target_indices[1]]).item()\n",
    "            difference_matrix[layer, position] = logit_diff\n",
    "\n",
    "    # Visualize heatmap without the first row\n",
    "    plt.figure(figsize=(10, 8))  # Ensure a clean figure\n",
    "    plt.matshow(difference_matrix[1:, :], cmap=\"Greys_r\", fignum=False)  # Exclude the first row\n",
    "    plt.colorbar(label=\"Logit Difference\")\n",
    "    plt.title(f\"Logit Differences for Tokens: {', '.join(target_tokens)}\")\n",
    "    plt.xlabel(\"Position in Sequence\")\n",
    "    plt.ylabel(\"Layer\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text = \"Michelle Jones was a top-notch student. Michelle\"\n",
    "corrupted_text = \"Michelle Smith was a top-notch student. Michelle\"\n",
    "target_tokens = [\"Smith\", \"Jones\"]\n",
    "\n",
    "# Analyze embeddings\n",
    "analyze_embeddings(model, bpe, clean_text, corrupted_text, target_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text = \"Jessica Jones was a top-notch student. Michelle\"\n",
    "corrupted_text = \"Michelle Smith was a top-notch student. Jessica\"\n",
    "target_tokens = [\"Smith\", \"Jones\"]\n",
    "\n",
    "# Analyze embeddings\n",
    "analyze_embeddings(model, bpe, clean_text, corrupted_text, target_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text = \"Messi is the best player. Messi\"\n",
    "corrupted_text = \"Messi is the worst player. Messi\"\n",
    "target_tokens = [\"worst\", \"best\"]\n",
    "\n",
    "# Analyze embeddings\n",
    "analyze_embeddings(model, bpe, clean_text, corrupted_text, target_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text = \"Once upon a time, there was a princess\"\n",
    "corrupted_text = \"Once upon a time, there was a dragon\"\n",
    "target_tokens = [\"dragon\",\"princess\"]\n",
    "\n",
    "# Analyze embeddings\n",
    "analyze_embeddings(model, bpe, clean_text, corrupted_text, target_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text = \"Thomas Green was an excellent musician. Thomas\"\n",
    "corrupted_text = \"Thomas White was an excellent musician. Thomas\"\n",
    "target_tokens = [\"White\", \"Green\"]\n",
    "\n",
    "# Analyze embeddings\n",
    "analyze_embeddings(model, bpe, clean_text, corrupted_text, target_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text = \"Thomas Green was an excellent musician. Thomas\"\n",
    "corrupted_text = \"Thomas Green was an excellent magician. Thomas\"\n",
    "target_tokens = [\"musician\", \"magician\"]\n",
    "\n",
    "# Analyze embeddings\n",
    "analyze_embeddings(model, bpe, clean_text, corrupted_text, target_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text = \"Albert Green was an excellent musician. Thomas\"\n",
    "corrupted_text = \"Thomas Green was an excellent musician. Albert\"\n",
    "target_tokens = [\"Albert\", \"Thomas\"]\n",
    "\n",
    "# Analyze embeddings\n",
    "analyze_embeddings(model, bpe, clean_text, corrupted_text, target_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text = \"I went to the bank to deposit money. The bank\"\n",
    "corrupted_text = \"I went to the bank to catch fish.The bank\"\n",
    "target_tokens = [\"catch\", \"deposit\"]\n",
    "\n",
    "# Analyze embeddings\n",
    "analyze_embeddings(model, bpe, clean_text, corrupted_text, target_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
